{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaaff6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede03a0f",
   "metadata": {},
   "source": [
    "## Pong\n",
    "\n",
    "Goal is to play the pong game, beating the machine using RL, the different variants that can be tried are as below - \n",
    "1. Policy Gradient\n",
    "2. Deep Q Learning\n",
    "3. Take action every 4 frames instead of every frame to speed up time\n",
    "4. Memory buffer - use last N (say 100) games to update model instead of just using the last game\n",
    "5. Linear annealing\n",
    "6. Remove first 20 frames as the ball is not there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a2d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# function to convert each image into lesser size\n",
    "def prepro(I):\n",
    "    # preprocess each frame for learning\n",
    "    # save some memory and computation\n",
    "    # pre-process the image from a 210x160x3 uint8 frame into an (80x80) float array \n",
    "    I = I[35:195,:,:].copy() # crop the top of the image...score image doesn't matter for how to play\n",
    "    I = I[::2,::2,0].copy()\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return np.array(I.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f535087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get discounted reward - unique to policy gradient\n",
    "def discount_rewards(r):\n",
    "    # take 1D float array of rewards and compute discounted reward\n",
    "    # gym returns a reward with every single frame.  most of those rewards are 0\n",
    "    # sometimes they're 1 or -1 if we win or lose a point in that specific frame\n",
    "    # we want non-0 rewards for every frame. \n",
    "    # so take each frame, figure out if we eventually won the corresponding point or not\n",
    "    # if so make the reward positive, if not negative\n",
    "    # but more recent actions (relative to the frame where the point is awarded) are more \n",
    "    # impactful to the score that frames a long time ago, so discount rewards...\n",
    "    \n",
    "    delt = 0.99 # discount factor\n",
    "    nr = r.shape[0]\n",
    "    # we want to change all those zeros into discounted values of the next reward (this is the value function!)\n",
    "    discounted_r = np.zeros(nr)\n",
    "    \n",
    "    for t in range(nr):\n",
    "        # start at the end\n",
    "        if r[nr-t-1] > 0: # if you won a point in this frame we want a good reward\n",
    "            discounted_r[nr-t-1] = 1 \n",
    "        elif r[nr-t-1] < 0: # if we lost the point we want a bad reward\n",
    "            discounted_r[nr-t-1] = -1\n",
    "        elif t==0: # this is just for error catching...at t==0 r[nr-t-1] should have already been + or -...\n",
    "            discounted_r[nr-t-1] = 0\n",
    "        elif discounted_r[nr-t-1] == 0: # otherwise you want to look at the next reward value and discount it\n",
    "            discounted_r[nr-t-1] = delt*discounted_r[nr-t]\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900364fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model creation\n",
    "def create_model(height,width,channels):\n",
    "    # we cannot simply have 3 output nodes because we want to put a weight on each node's impact to the objective\n",
    "    # that is different for each data point.  the only way to achieve this is to have 3 output layers, each having 1 node\n",
    "    # the effect is the same, just the way TF/keras handles weights is different\n",
    "    imp = Input(shape=(height,width,channels))\n",
    "    mid = Conv2D(16,(8,8),strides=4,activation='relu')(imp)\n",
    "    mid = Conv2D(32,(4,4),strides=2,activation='relu')(mid)\n",
    "    mid = Flatten()(mid)\n",
    "    mid = Dense(256,activation='relu')(mid)\n",
    "    out0 = Dense(3,activation='softmax')(mid)\n",
    "    model = Model(imp,out0) \n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996e63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 80, 80, 4)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 19, 19, 16)        4112      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 32)          8224      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537,651\n",
      "Trainable params: 537,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "frames_to_net = 4              # how many previous frames will we feed the NN\n",
    "possible_actions = [0,2,3]\n",
    "mod = create_model(80,80,frames_to_net)\n",
    "mod.call = tf.function(mod.call,experimental_relax_shapes=True)\n",
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ed31a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first variant of playing the game\n",
    "# Deep Q learning\n",
    "# other feature\n",
    "# \n",
    "def play1game(model,ep):\n",
    "    env0 = gym.make(\"Pong-v0\")\n",
    "    pix = env0.reset()\n",
    "    pix = prepro(pix)\n",
    "    frames_this_game = 0\n",
    "    feed = np.zeros((1,80,80,frames_to_net))\n",
    "    feed[0,:,:,0] = pix.copy() # 0 is the most recent frame t; 1 the previous one t+1 and so one\n",
    "    \n",
    "    frame_array = []\n",
    "    action_array = []\n",
    "    reward_array = []\n",
    "    \n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.random() < ep:\n",
    "            action = np.random.choice(3)\n",
    "        else:\n",
    "            vf = mod(feed,training=False)\n",
    "            vf = [vf[0][0,0].numpy(),vf[1][0,0].numpy(),vf[2][0,0].numpy()]\n",
    "            action = np.argmax(vf)\n",
    "        action0 = possible_actions[action]\n",
    "        pix_new, reward, done, info = env0.step(action0)\n",
    "        frame_array.append(pix)\n",
    "        action_array.append(action)\n",
    "        reward_array.append(reward)\n",
    "        pix = prepro(pix_new)\n",
    "        frames_this_game += 1\n",
    "\n",
    "        for f in range(1,frames_to_net):\n",
    "            feed[0,:,:,frames_to_net-f] = feed[0,:,:,frames_to_net-f-1].copy()\n",
    "        feed[0,:,:,0] = pix.copy()\n",
    "        score += reward\n",
    "    return frame_array, action_array, reward_array, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "963ea283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def play1game(model):\n",
    "#     env0 = gym.make(\"Pong-v0\")\n",
    "#     pix = env0.reset()\n",
    "#     pix = prepro(pix)\n",
    "#     frames_this_game = 0\n",
    "#     feed = np.zeros((1,80,80,frames_to_net))\n",
    "#     feed[0,:,:,0] = pix.copy()\n",
    "    \n",
    "#     frame_array = []\n",
    "#     action_array = []\n",
    "#     reward_array = []\n",
    "    \n",
    "#     score = 0\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         vf = model(feed,training=False).numpy()[0]\n",
    "#         action = np.random.choice(3,p=vf)\n",
    "        \n",
    "#         action0 = possible_actions[action]\n",
    "#         pix_new, reward, done, info = env0.step(action0)\n",
    "#         frame_array.append(pix)\n",
    "#         action_array.append(action)\n",
    "#         reward_array.append(reward)\n",
    "#         pix = prepro(pix_new)\n",
    "#         frames_this_game += 1\n",
    "\n",
    "#         for f in range(1,frames_to_net):\n",
    "#             feed[0,:,:,frames_to_net-f] = feed[0,:,:,frames_to_net-f-1].copy()\n",
    "#         feed[0,:,:,0] = pix.copy()\n",
    "#         score += reward\n",
    "        \n",
    "#     return frame_array, action_array, reward_array, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24443f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear annealing with same actions for 4 games\n",
    "def play2game(model, eps):\n",
    "    env0 = gym.make(\"Pong-v0\")\n",
    "    pix = env0.reset()\n",
    "    pix = prepro(pix)\n",
    "    frames_this_game = 0\n",
    "    feed = np.zeros((1,80,80,frames_to_net))\n",
    "    feed[0,:,:,0] = pix.copy()\n",
    "    \n",
    "    frame_array = []\n",
    "    action_array = []\n",
    "    reward_array = []\n",
    "    \n",
    "    score = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        # updating action every 4 frames\n",
    "        # but taking that action for the 4 frames just not using the prediction for action\n",
    "        if frames_this_game % 4 == 0:\n",
    "            vf = model(feed,training=False).numpy()[0]\n",
    "            # print(vf)\n",
    "            if np.random.random() < eps:\n",
    "                action = np.random.choice(3,p=vf)\n",
    "                action0 = possible_actions[action]\n",
    "            else:\n",
    "                action = np.argmax(vf)\n",
    "                action0 = possible_actions[action]\n",
    "                \n",
    "            # updating epsilon for picking\n",
    "            # if eps > eps_e:\n",
    "            #    eps -= eps_d             \n",
    "        \n",
    "        pix_new, reward, done, info = env0.step(action0)\n",
    "        frame_array.append(pix)\n",
    "        action_array.append(action)\n",
    "        reward_array.append(reward)\n",
    "        pix = prepro(pix_new)\n",
    "        frames_this_game += 1\n",
    "\n",
    "        for f in range(1,frames_to_net):\n",
    "            feed[0,:,:,frames_to_net-f] = feed[0,:,:,frames_to_net-f-1].copy()\n",
    "        feed[0,:,:,0] = pix.copy()\n",
    "        score += reward\n",
    "        \n",
    "    return frame_array, action_array, reward_array, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8122274",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "play1game() missing 1 required positional argument: 'ep'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-162817c5ba29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mframes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay1game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: play1game() missing 1 required positional argument: 'ep'"
     ]
    }
   ],
   "source": [
    "frames, actions, rewards, score = play1game(mod)\n",
    "print(np.mean(np.array(actions)==0),np.mean(np.array(actions)==1),np.mean(np.array(actions)==2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e60d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngames = 20\n",
    "# batches_per_epoch = 50\n",
    "n_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2609ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for game in range(ngames):\n",
    "#     start = time.time()\n",
    "#     frames, actions, rewards, score = play1game(mod)\n",
    "#     rewards = np.array(rewards)\n",
    "#     actions = np.array(actions)\n",
    "#     nframes = len(frames)\n",
    "#     current_frames = np.zeros((nframes,80,80,frames_to_net))\n",
    "    \n",
    "#     disc_rewards = discount_rewards(rewards)\n",
    "  \n",
    "#     for grab in range(nframes):\n",
    "#         for f in range(frames_to_net):\n",
    "#             if grab-f > 0:\n",
    "#                 current_frames[grab,:,:,f] = frames[grab-f].copy()\n",
    "  \n",
    "#     mod.fit(current_frames,actions,epochs=1,batch_size=n_batch,verbose=0,sample_weight=disc_rewards,use_multiprocessing=True)\n",
    "    \n",
    "#     # for some reason colab memory was blowing up...this may have fixed it?\n",
    "# #     del rewards\n",
    "# #     del actions\n",
    "# #     del frames\n",
    "# #     del current_frames\n",
    "# #     del disc_rewards\n",
    "#     stop = time.time()\n",
    "#     print([game, score, stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b70fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngames = 1000\n",
    "epsvec = np.linspace(1,0.05,ngames)\n",
    "ngames = 10\n",
    "delt = 0.99\n",
    "nbatch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f40da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for game in range(ngames):\n",
    "    start = time.time()\n",
    "    frames, actions, rewards, score = play2game(mod,epsvec[game])\n",
    "\n",
    "    nframes = len(frames)\n",
    "    current_frames = np.zeros((nframes,80,80,frames_to_net))\n",
    "    future_frames = np.zeros((nframes,80,80,frames_to_net))\n",
    "  \n",
    "    # creating frame - current t (for prediction) and t+1 (for truth)\n",
    "    for grab in range(nframes):\n",
    "        for f in range(frames_to_net):\n",
    "            if grab-f > 0:\n",
    "                current_frames[grab,:,:,f] = frames[grab-f].copy()\n",
    "            if (grab-f+1 > 0) & (grab-f+1 < (nframes-1)):\n",
    "                future_frames[grab,:,:,f] = frames[grab-f+1].copy()\n",
    "    target_vf = mod.predict(future_frames)\n",
    "\n",
    "    # vectors of truth\n",
    "    y0 = np.zeros((nframes,1))\n",
    "    y1 = np.zeros((nframes,1))\n",
    "    y2 = np.zeros((nframes,1))\n",
    "    \n",
    "    # weight for training neural network based on the \"truth\"\n",
    "    weight0 = np.zeros(nframes)\n",
    "    weight1 = np.zeros(nframes)\n",
    "    weight2 = np.zeros(nframes)\n",
    "  \n",
    "\n",
    "    for grab in range(nframes):\n",
    "        rhs = rewards[grab]\n",
    "        # terminal condition will be when we win a game\n",
    "        # \n",
    "        if rhs == 0:\n",
    "            rhs = delt*np.max([target_vf[0][grab],target_vf[1][grab],target_vf[2][grab]])\n",
    "        if actions[grab] == 0:\n",
    "            y0[grab,0] = rhs\n",
    "            weight0[grab] = 1\n",
    "        elif actions[grab] == 1:\n",
    "            y1[grab,0] = rhs\n",
    "            weight1[grab] = 1\n",
    "        else:\n",
    "            y2[grab,0] = rhs\n",
    "            weight2[grab] = 1\n",
    "  \n",
    "    mod.fit(current_frames,[y0,y1,y2],epochs=1,batch_size=nbatch,verbose=0,sample_weight={'out0':weight0,'out1':weight1,'out2':weight2},use_multiprocessing=True)\n",
    "    stop = time.time()\n",
    "    print([game, score, epsvec[game], stop-start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6280e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb4ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
